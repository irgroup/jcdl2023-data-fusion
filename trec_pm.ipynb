{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGTIF40fmLI"
      },
      "source": [
        "This notebook compares the rankings of systems (ROS) between editorial relevance labels and citation counts. The code below mainly uses the data from the \"Scientific Abstracts\" task at TREC Precision Medicine 2017. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKo8862xY1ez"
      },
      "source": [
        "**Download TREC Precision Medicine run files.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElomTfbOYlKM",
        "outputId": "153d3807-2be5-4c71-c627-22ccd63a5414"
      },
      "outputs": [],
      "source": [
        "!wget -O trec-pm.tar.xz https://th-koeln.sciebo.de/s/JTTV4fxFmuCGMeY/download trec-pm.tar.xz\n",
        "!tar -xf trec-pm.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdk1KXL9bIWX"
      },
      "source": [
        "**The directory includes the qrels and all runs submitted to the \"Scientific Abstracts\" and \"Clinical Trials\" tracks at TREC PM 2017-19**\n",
        "\n",
        "see also: https://trec.nist.gov/data/precmed.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_cfFGlwax3G",
        "outputId": "7b3024de-e079-495e-ef74-c1077b37d171"
      },
      "outputs": [],
      "source": [
        "!ls trec-pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Hon9oZZEzJ"
      },
      "source": [
        "**Download Dirk's citation and altmetric data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDkdXUIY__S",
        "outputId": "940691e2-b8fb-4247-a039-5656067d1fa4"
      },
      "outputs": [],
      "source": [
        "!wget -O bibliometric.tar.xz https://th-koeln.sciebo.de/s/BRolGxMzrCipoTT/download\n",
        "!tar -xf bibliometric.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install needed python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install --yes --file requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "186vihTtdhxZ"
      },
      "source": [
        "**Make a qrels file from the citation data. The following code uses simple criteria to make multi-graded labels from the citation count and writes them into a file 'qrels.cite'.**\n",
        "- 2: if the number of citations is higher than twice the mean of all citations\n",
        "- 0: if the number of citations is lower than the mean of all citations\n",
        "- 1: the ones in between"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtUlk-D7bPQg",
        "outputId": "7a4ea140-ff79-4622-9051-2c1f3dde692d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv('STI_Ergebnisse_final.txt', sep='\\t')\n",
        "\n",
        "_df = df[df['TC'].notna()]\n",
        "_df = _df[_df['TOPIC'].str.contains('2017', regex=False)]\n",
        "_df = _df[['TOPIC','PUBMED_ID', 'TC']]\n",
        "thresh = df[df['TC'].notna()]['TC'].mean()\n",
        "\n",
        "with open('qrels.cite', 'w') as f_out:\n",
        "\n",
        "    for row in _df.iterrows():\n",
        "\n",
        "        topic = row[1]['TOPIC'].split('-')[1]\n",
        "        pubmed_id = row[1]['PUBMED_ID']\n",
        "        citation_cnt = row[1]['TC']\n",
        "        rel = 1\n",
        "        \n",
        "        if citation_cnt >= 2*thresh:\n",
        "            rel = 2\n",
        "        if citation_cnt < thresh:\n",
        "            rel = 0\n",
        "            \n",
        "        line_out = ' '.join([topic, '0', str(pubmed_id), str(rel), '\\n'])\n",
        "                \n",
        "        f_out.write(line_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_53U0T7dvhQ"
      },
      "source": [
        "**Extract the run files and write them into a new directory.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2unECHZ2bZI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "def extract_runs(dir_in, dir_out):\n",
        "\n",
        "    os.makedirs(dir_out, exist_ok=True)\n",
        "\n",
        "    for root, dirs, files in os.walk(dir_in):\n",
        "        for file in files:\n",
        "            if file.endswith(\".gz\"):\n",
        "                run_name = file.split('.')[1]\n",
        "                with gzip.open(os.path.join(root, file), 'rb') as f_in:\n",
        "                    file_content = f_in.read()\n",
        "                    with open(dir_out + '/' + run_name, 'wb') as f_out:\n",
        "                        f_out.write(file_content) \n",
        "                          \n",
        "DIR_IN = 'trec-pm/trec-pm-2017-abstracts' \n",
        "DIR_OUT = 'runs/trec-pm-2017-abstracts'    \n",
        "                    \n",
        "extract_runs(DIR_IN, DIR_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeCDGTHqeaLu"
      },
      "source": [
        "**Install the super-fast evaluation toolkit ranx, which implements some trec_eval measures with the help of Python and numba.**\n",
        "\n",
        "see also: https://github.com/AmenRa/ranx or https://amenra.github.io/ranx/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE_42_5XblzA",
        "outputId": "c49f4dcc-28a1-4636-bab2-8ea3c73c1452"
      },
      "outputs": [],
      "source": [
        "!pip install ranx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiKaoZSoemPT"
      },
      "source": [
        "**Make a reference system of rankings (ROS) from the qrels of the \"Scientific Abstracts\" task at TREC PM 2017.**\n",
        "\n",
        "The first time, it takes a while to run ranx as it needs to compile the source code. Later executions will run much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geF2aFQJbkOE",
        "outputId": "e56ba616-e590-4f20-adf6-2232f1e14df3"
      },
      "outputs": [],
      "source": [
        "from ranx import Qrels, Run, evaluate, compare\n",
        "\n",
        "DIR_RUN = DIR_OUT\n",
        "PATH_QRELS = \"trec-pm/trec-pm-2017-abstracts/qrels-final-abstracts.txt\"\n",
        "\n",
        "qrels = Qrels.from_file(PATH_QRELS, kind=\"trec\")\n",
        "\n",
        "ros_ref = {}\n",
        "\n",
        "for root, dirs, files in os.walk(DIR_RUN):\n",
        "    for file in files:\n",
        "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
        "        score = evaluate(qrels, run, \"ndcg@5\")\n",
        "        ros_ref[file] = score\n",
        "\n",
        "ros_ref = dict(sorted(ros_ref.items(), key=lambda item: item[1], reverse=True))\n",
        "ros_ref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIn0j_u7fCsg"
      },
      "source": [
        "**Make the corresponding ROS based on citation data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgMQWOAb0bq",
        "outputId": "196399d9-e28f-43ef-a6d1-c5bd22eceb79"
      },
      "outputs": [],
      "source": [
        "PATH_QRELS_CITE = \"qrels.cite\"\n",
        "\n",
        "qrels = Qrels.from_file(PATH_QRELS_CITE, kind=\"trec\")\n",
        "\n",
        "ros_cite = {}\n",
        "\n",
        "for root, dirs, files in os.walk(DIR_RUN):\n",
        "    for file in files:\n",
        "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
        "        score = evaluate(qrels, run, \"ndcg@5\")\n",
        "        ros_cite[file] = score\n",
        "\n",
        "ros_cite = dict(sorted(ros_cite.items(), key=lambda item: item[1], reverse=True))\n",
        "ros_cite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCwcEYwofNVv"
      },
      "source": [
        "**Determine Kendall's tau between the ROS.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1ET7zvb3ud",
        "outputId": "236a1665-50a0-4e77-942a-a238f429c80b"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "tau, p_value = stats.kendalltau(list(ros_ref.keys()), list(ros_cite.keys()))\n",
        "tau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Classifier preparation**\n",
        "- Prepare data, split columns\n",
        "- Drop columns from data which contain non-numeric data\n",
        "- Todo: transform non-numeric data to numeric data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['PUBMED_ID', 'RELEVANCE', 'JAHR', 'PY', 'PT', 'DT', 'SO', 'TC', 'IF',\n",
            "       'RL', 'ATTENTION_SCORE', 'NEWS', 'BLOG', 'POLICY', 'PATENT', 'TWITTER',\n",
            "       'PEER_REVIEW', 'WEIBO', 'FACEBOOK', 'WIKIPEDIA', 'GOOGLE', 'LINKEDIN',\n",
            "       'REDDIT', 'PINTEREST', 'F1000', 'Q_A', 'VIDEO', 'SYLLABI', 'MENDELEY',\n",
            "       'DIMENSIONS', 'TCpY', 'TOPIC_ID', 'TOPIC_YEAR'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "_df2 = pd.read_csv('STI_Ergebnisse_final.txt', sep='\\t', low_memory=False)\n",
        "\n",
        "# insert citations per year\n",
        "_df2['TCpY'] = _df2['TC']/(_df2['JAHR']-_df2['PY'])\n",
        "\n",
        "# separate topic into ID and year\n",
        "_df2['TOPIC_ID'] = _df2['TOPIC'].str[5:].astype(int)\n",
        "_df2['TOPIC_YEAR'] = _df2['TOPIC'].str[:4].astype(int)\n",
        "\n",
        "# replace non-numeric values with numeric ones\n",
        "encoder = preprocessing.OrdinalEncoder()\n",
        "_df2[['PT', 'DT', 'SO']] = encoder.fit_transform(_df2[['PT', 'DT', 'SO']])\n",
        "\n",
        "# todo: handle columns 'DOMAIN', 'FIELD', 'SUBFIELD'\n",
        "\n",
        "# drop non-numeric columns\n",
        "drop_columns = ['UT', 'DOI', 'ISSN', 'ARXIVID', 'DOMAIN', 'FIELD', 'SUBFIELD', 'TOPIC']\n",
        "_df2.drop(labels=drop_columns, axis=1, inplace=True)\n",
        "\n",
        "# reformat strings to floats\n",
        "_df2['RL'] = _df2['RL'].str.replace(',', '').astype(float)\n",
        "_df2['IF'] = _df2['IF'].str.replace(',', '').astype(float)\n",
        "\n",
        "# drop all rows where an inf or nan value occurs\n",
        "_df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "_df2.dropna(inplace=True)\n",
        "\n",
        "# scale values\n",
        "# todo: handle all values\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "_df2[['JAHR', 'PY','TC', 'IF', 'RL', 'ATTENTION_SCORE', 'NEWS', 'BLOG', 'POLICY', 'PATENT', 'TWITTER',\n",
        "       'PEER_REVIEW', 'WEIBO', 'FACEBOOK', 'WIKIPEDIA', 'GOOGLE', 'LINKEDIN',\n",
        "       'REDDIT', 'PINTEREST', 'F1000', 'Q_A', 'VIDEO', 'SYLLABI', 'MENDELEY',\n",
        "       'DIMENSIONS', 'TCpY', 'TOPIC_YEAR' ]] = scaler.fit_transform(_df2[['JAHR', 'PY','TC', 'IF', 'RL', 'ATTENTION_SCORE', 'NEWS', 'BLOG', 'POLICY', 'PATENT', 'TWITTER',\n",
        "       'PEER_REVIEW', 'WEIBO', 'FACEBOOK', 'WIKIPEDIA', 'GOOGLE', 'LINKEDIN',\n",
        "       'REDDIT', 'PINTEREST', 'F1000', 'Q_A', 'VIDEO', 'SYLLABI', 'MENDELEY',\n",
        "       'DIMENSIONS', 'TCpY', 'TOPIC_YEAR' ]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# relevance labels\n",
        "rel = _df2['RELEVANCE']\n",
        "y_train = rel.iloc[0:20000]\n",
        "y_test = rel.iloc[20000:]\n",
        "\n",
        "_df2.drop(labels='RELEVANCE', axis=1, inplace=True)\n",
        "\n",
        "# actual data\n",
        "X_train = _df2.values[0:20000]\n",
        "X_test = _df2.values[20000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train/validation split of data, try different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.05\n",
            "Accuracy score (training): 0.771\n",
            "Accuracy score (validation): 0.772\n",
            "Learning rate:  0.075\n",
            "Accuracy score (training): 0.776\n",
            "Accuracy score (validation): 0.774\n",
            "Learning rate:  0.1\n",
            "Accuracy score (training): 0.780\n",
            "Accuracy score (validation): 0.775\n",
            "Learning rate:  0.25\n",
            "Accuracy score (training): 0.803\n",
            "Accuracy score (validation): 0.778\n",
            "Learning rate:  0.5\n",
            "Accuracy score (training): 0.834\n",
            "Accuracy score (validation): 0.776\n",
            "Learning rate:  0.75\n",
            "Accuracy score (training): 0.853\n",
            "Accuracy score (validation): 0.770\n",
            "Learning rate:  1\n",
            "Accuracy score (training): 0.863\n",
            "Accuracy score (validation): 0.762\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "state = 123  \n",
        "test_size = 0.30\n",
        "  \n",
        "# further split into train and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=state)\n",
        "\n",
        "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "\n",
        "for learning_rate in lr_list:\n",
        "    gb_clf = GradientBoostingClassifier(learning_rate=learning_rate, random_state=0)\n",
        "    gb_clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Learning rate: \", learning_rate)\n",
        "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
        "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_val, y_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test classifier's performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[7997  272   87]\n",
            " [ 979  274   17]\n",
            " [1221  116   66]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.96      0.86      8356\n",
            "           1       0.41      0.22      0.28      1270\n",
            "           2       0.39      0.05      0.08      1403\n",
            "\n",
            "    accuracy                           0.76     11029\n",
            "   macro avg       0.53      0.41      0.41     11029\n",
            "weighted avg       0.69      0.76      0.70     11029\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "gb_clf2 = GradientBoostingClassifier(learning_rate=0.25, random_state=0)\n",
        "gb_clf2.fit(X_train, y_train)\n",
        "predictions = gb_clf2.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM3SXFmlQvCB8xZK+C1EHLH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c33ba880b3fd3336bbefee266d2ac64afa5e1bdaf0f5e0f6d27bed480abb3f7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
