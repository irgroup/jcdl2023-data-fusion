{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGTIF40fmLI"
      },
      "source": [
        "This notebook compares the rankings of systems (ROS) between editorial relevance labels and citation counts. The code below mainly uses the data from the \"Scientific Abstracts\" task at TREC Precision Medicine 2017. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKo8862xY1ez"
      },
      "source": [
        "**Download TREC Precision Medicine run files.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElomTfbOYlKM",
        "outputId": "153d3807-2be5-4c71-c627-22ccd63a5414"
      },
      "outputs": [],
      "source": [
        "!wget -O trec-pm.tar.xz https://th-koeln.sciebo.de/s/JTTV4fxFmuCGMeY/download trec-pm.tar.xz\n",
        "!tar -xf trec-pm.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdk1KXL9bIWX"
      },
      "source": [
        "**The directory includes the qrels and all runs submitted to the \"Scientific Abstracts\" and \"Clinical Trials\" tracks at TREC PM 2017-19**\n",
        "\n",
        "see also: https://trec.nist.gov/data/precmed.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_cfFGlwax3G",
        "outputId": "7b3024de-e079-495e-ef74-c1077b37d171"
      },
      "outputs": [],
      "source": [
        "!ls trec-pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Hon9oZZEzJ"
      },
      "source": [
        "**Download Dirk's citation and altmetric data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDkdXUIY__S",
        "outputId": "940691e2-b8fb-4247-a039-5656067d1fa4"
      },
      "outputs": [],
      "source": [
        "!wget -O bibliometric.tar.xz https://th-koeln.sciebo.de/s/BRolGxMzrCipoTT/download\n",
        "!tar -xf bibliometric.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install needed python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install --yes --file requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "186vihTtdhxZ"
      },
      "source": [
        "**Make a qrels file from the citation data. The following code uses simple criteria to make multi-graded labels from the citation count and writes them into a file 'qrels.cite'.**\n",
        "- 2: if the number of citations is higher than twice the mean of all citations\n",
        "- 0: if the number of citations is lower than the mean of all citations\n",
        "- 1: the ones in between"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtUlk-D7bPQg",
        "outputId": "7a4ea140-ff79-4622-9051-2c1f3dde692d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/6r/m6x_qnvj5cs5fyw9sw9xczfw0000gn/T/ipykernel_14996/1713252840.py:3: DtypeWarning: Columns (2,5,7,8,9,10,12,13,14,15,16,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('STI_Ergebnisse_final.txt', sep='\\t')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv('STI_Ergebnisse_final.txt', sep='\\t')\n",
        "\n",
        "_df = df[df['TC'].notna()]\n",
        "_df = _df[_df['TOPIC'].str.contains('2017', regex=False)]\n",
        "_df = _df[['TOPIC','PUBMED_ID', 'TC']]\n",
        "thresh = df[df['TC'].notna()]['TC'].mean()\n",
        "\n",
        "with open('qrels.cite', 'w') as f_out:\n",
        "\n",
        "    for row in _df.iterrows():\n",
        "\n",
        "        topic = row[1]['TOPIC'].split('-')[1]\n",
        "        pubmed_id = row[1]['PUBMED_ID']\n",
        "        citation_cnt = row[1]['TC']\n",
        "        rel = 1\n",
        "        \n",
        "        if citation_cnt >= 2*thresh:\n",
        "            rel = 2\n",
        "        if citation_cnt < thresh:\n",
        "            rel = 0\n",
        "            \n",
        "        line_out = ' '.join([topic, '0', str(pubmed_id), str(rel), '\\n'])\n",
        "                \n",
        "        f_out.write(line_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/6r/m6x_qnvj5cs5fyw9sw9xczfw0000gn/T/ipykernel_14996/2513010742.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  _df2['TCbY'] = _df2['TC']/(_df2['JAHR']-_df2['PY'])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "_df2 = df[df['TC'].notna()]\n",
        "_df2['TCbY'] = _df2['TC']/(_df2['JAHR']-_df2['PY'])\n",
        "_df2 = _df2[_df2['ATTENTION_SCORE'].notna()]\n",
        "\n",
        "drop_columns = ['UT', 'SO', 'DOI', 'ISSN', 'ARXIVID', 'DOMAIN', 'FIELD', 'PT', 'DT', 'SUBFIELD', 'TOPIC', 'PUBMED_ID']\n",
        "_df2.drop(labels=drop_columns, axis=1, inplace=True)\n",
        "\n",
        "_df2['RL'] = _df2['RL'].str.replace(',', '').astype(float)\n",
        "_df2['IF'] = _df2['IF'].str.replace(',', '').astype(float)\n",
        "\n",
        "# drop all rows where an inf or nan value occurs\n",
        "_df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "_df2 = _df2.dropna()\n",
        "\n",
        "# relevance labels\n",
        "rel = _df2['RELEVANCE']\n",
        "y_train = rel.iloc[0:1000]\n",
        "y_test = rel.iloc[1000:]\n",
        "\n",
        "_df2.drop(labels='RELEVANCE', axis=1, inplace=True)\n",
        "\n",
        "# actual data\n",
        "X_train = _df2.values[0:1000]\n",
        "X_test = _df2.values[1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.05\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.773\n",
            "Learning rate:  0.075\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.770\n",
            "Learning rate:  0.1\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.763\n",
            "Learning rate:  0.25\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.753\n",
            "Learning rate:  0.5\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.760\n",
            "Learning rate:  0.75\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.760\n",
            "Learning rate:  1\n",
            "Accuracy score (training): 0.964\n",
            "Accuracy score (validation): 0.753\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "state = 123  \n",
        "test_size = 0.30\n",
        "  \n",
        "# further split into train and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=state)\n",
        "\n",
        "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "\n",
        "for learning_rate in lr_list:\n",
        "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=20, max_depth=20, random_state=0)\n",
        "    gb_clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Learning rate: \", learning_rate)\n",
        "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
        "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_val, y_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[22080   144   350]\n",
            " [ 3397    26   170]\n",
            " [ 3699    25   138]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.98      0.85     22574\n",
            "           1       0.13      0.01      0.01      3593\n",
            "           2       0.21      0.04      0.06      3862\n",
            "\n",
            "    accuracy                           0.74     30029\n",
            "   macro avg       0.37      0.34      0.31     30029\n",
            "weighted avg       0.61      0.74      0.65     30029\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.05, max_features=20, max_depth=20, random_state=0)\n",
        "gb_clf2.fit(X_train, y_train)\n",
        "predictions = gb_clf2.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_53U0T7dvhQ"
      },
      "source": [
        "**Extract the run files and write them into a new directory.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2unECHZ2bZI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "def extract_runs(dir_in, dir_out):\n",
        "\n",
        "    os.makedirs(dir_out, exist_ok=True)\n",
        "\n",
        "    for root, dirs, files in os.walk(dir_in):\n",
        "        for file in files:\n",
        "            if file.endswith(\".gz\"):\n",
        "                run_name = file.split('.')[1]\n",
        "                with gzip.open(os.path.join(root, file), 'rb') as f_in:\n",
        "                    file_content = f_in.read()\n",
        "                    with open(dir_out + '/' + run_name, 'wb') as f_out:\n",
        "                        f_out.write(file_content) \n",
        "                          \n",
        "DIR_IN = 'trec-pm/trec-pm-2017-abstracts' \n",
        "DIR_OUT = 'runs/trec-pm-2017-abstracts'    \n",
        "                    \n",
        "extract_runs(DIR_IN, DIR_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeCDGTHqeaLu"
      },
      "source": [
        "**Install the super-fast evaluation toolkit ranx, which implements some trec_eval measures with the help of Python and numba.**\n",
        "\n",
        "see also: https://github.com/AmenRa/ranx or https://amenra.github.io/ranx/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE_42_5XblzA",
        "outputId": "c49f4dcc-28a1-4636-bab2-8ea3c73c1452"
      },
      "outputs": [],
      "source": [
        "!pip install ranx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiKaoZSoemPT"
      },
      "source": [
        "**Make a reference system of rankings (ROS) from the qrels of the \"Scientific Abstracts\" task at TREC PM 2017.**\n",
        "\n",
        "The first time, it takes a while to run ranx as it needs to compile the source code. Later executions will run much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geF2aFQJbkOE",
        "outputId": "e56ba616-e590-4f20-adf6-2232f1e14df3"
      },
      "outputs": [],
      "source": [
        "from ranx import Qrels, Run, evaluate, compare\n",
        "\n",
        "DIR_RUN = DIR_OUT\n",
        "PATH_QRELS = \"trec-pm/trec-pm-2017-abstracts/qrels-final-abstracts.txt\"\n",
        "\n",
        "qrels = Qrels.from_file(PATH_QRELS, kind=\"trec\")\n",
        "\n",
        "ros_ref = {}\n",
        "\n",
        "for root, dirs, files in os.walk(DIR_RUN):\n",
        "    for file in files:\n",
        "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
        "        score = evaluate(qrels, run, \"ndcg@5\")\n",
        "        ros_ref[file] = score\n",
        "\n",
        "ros_ref = dict(sorted(ros_ref.items(), key=lambda item: item[1], reverse=True))\n",
        "ros_ref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIn0j_u7fCsg"
      },
      "source": [
        "**Make the corresponding ROS based on citation data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgMQWOAb0bq",
        "outputId": "196399d9-e28f-43ef-a6d1-c5bd22eceb79"
      },
      "outputs": [],
      "source": [
        "PATH_QRELS_CITE = \"qrels.cite\"\n",
        "\n",
        "qrels = Qrels.from_file(PATH_QRELS_CITE, kind=\"trec\")\n",
        "\n",
        "ros_cite = {}\n",
        "\n",
        "for root, dirs, files in os.walk(DIR_RUN):\n",
        "    for file in files:\n",
        "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
        "        score = evaluate(qrels, run, \"ndcg@5\")\n",
        "        ros_cite[file] = score\n",
        "\n",
        "ros_cite = dict(sorted(ros_cite.items(), key=lambda item: item[1], reverse=True))\n",
        "ros_cite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCwcEYwofNVv"
      },
      "source": [
        "**Determine Kendall's tau between the ROS.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1ET7zvb3ud",
        "outputId": "236a1665-50a0-4e77-942a-a238f429c80b"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "tau, p_value = stats.kendalltau(list(ros_ref.keys()), list(ros_cite.keys()))\n",
        "tau"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM3SXFmlQvCB8xZK+C1EHLH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c33ba880b3fd3336bbefee266d2ac64afa5e1bdaf0f5e0f6d27bed480abb3f7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
