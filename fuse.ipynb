{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ranx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O trec-pm.tar.xz https://th-koeln.sciebo.de/s/JTTV4fxFmuCGMeY/download\n",
    "!tar -xf trec-pm.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O bibliometric.tar.xz https://th-koeln.sciebo.de/s/BRolGxMzrCipoTT/download\n",
    "!tar -xf bibliometric.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "from ranx import Qrels, Run, evaluate, compare, fuse, optimize_fusion\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_and_qrels = { \n",
    "    '2017': {\n",
    "        'DIR_IN': 'trec-pm/trec-pm-2017-abstracts', \n",
    "        'DIR_RUN': 'runs/trec-pm-2017-abstracts',    \n",
    "        'PATH_QRELS': \"trec-pm/trec-pm-2017-abstracts/qrels-final-abstracts.txt\",\n",
    "        'DIR_CSV': 'experimental_results/trec-pm-2017-abstracts/',\n",
    "        'YEAR': 2017\n",
    "    }, \n",
    "    '2018': {\n",
    "        'DIR_IN': 'trec-pm/trec-pm-2018-abstracts', \n",
    "        'DIR_RUN': 'runs/trec-pm-2018-abstracts',\n",
    "        'PATH_QRELS': \"trec-pm/trec-pm-2018-abstracts/qrels-treceval-abstracts-2018-v2.txt\",\n",
    "        'DIR_CSV': 'experimental_results/trec-pm-2018-abstracts/',\n",
    "        'YEAR': 2018\n",
    "    },\n",
    "    '2019': {\n",
    "        'DIR_IN': 'trec-pm/trec-pm-2019-abstracts', \n",
    "        'DIR_RUN': 'runs/trec-pm-2019-abstracts',\n",
    "        'PATH_QRELS': \"trec-pm/trec-pm-2019-abstracts/qrels-treceval-abstracts.2019.txt\",\n",
    "        'DIR_CSV': 'experimental_results/trec-pm-2019-abstracts/',\n",
    "        'YEAR': 2019\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_runs(year_and_qrels):\n",
    "    os.makedirs(year_and_qrels['DIR_RUN'], exist_ok=True)\n",
    "\n",
    "    for root, dirs, files in os.walk(year_and_qrels['DIR_IN']):\n",
    "        for file in files:\n",
    "            if file.endswith(\".gz\"):\n",
    "                run_name = file.split('.')[1]\n",
    "                with gzip.open(os.path.join(root, file), 'rb') as f_in:\n",
    "                    file_content = f_in.read()\n",
    "                    file_out = year_and_qrels['DIR_RUN'] + '/' + run_name\n",
    "                    with open(file_out, 'wb') as f_out:\n",
    "                        f_out.write(file_content) \n",
    "                        if year_and_qrels['YEAR'] == 2019:\n",
    "                            !cat $file_out | cut -d\" \" -f1 > tmp; mv tmp $file_out\n",
    "                            \n",
    "for year in years_and_qrels:\n",
    "    extract_runs(years_and_qrels[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run(category, year=2017):\n",
    "    \n",
    "    run_names = {'TC': 'Citations (C)',\n",
    "                 'ATTENTION_SCORE': 'Altmetrics (A)',\n",
    "                 'RL': 'Research level (R)',\n",
    "                 'IF': 'Impact factor (I)',\n",
    "                 'PY': 'Publication year (P)'}\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('STI_Ergebnisse_final.txt', sep='\\t', low_memory=False)\n",
    "    _run = Run(name=run_names[category])\n",
    "    \n",
    "    _df = df[df[category].notna()]\n",
    "    _df = _df[_df['TOPIC'].str.contains(str(year), regex=False)]\n",
    "    _df = _df[['TOPIC','PUBMED_ID', category]]\n",
    "    \n",
    "    if category in ['IF', 'RL']:\n",
    "        _df[category] = _df[category].str.replace(',','.').astype(float)\n",
    "\n",
    "    for row in _df.iterrows():\n",
    "        topic = row[1]['TOPIC'].split('-')[1]\n",
    "        pubmed_id = row[1]['PUBMED_ID']\n",
    "        cnt = row[1][category]\n",
    "\n",
    "        if cnt > 0:\n",
    "            _run.add_score(str(topic), str(pubmed_id), float(cnt))\n",
    "    \n",
    "    return _run\n",
    "\n",
    "def get_random_run(qrels):\n",
    "    _run = Run(name='random')\n",
    "    \n",
    "    for topic, _qrels in dict(qrels).items():\n",
    "        for pubmed_id, label in _qrels.items():\n",
    "            _run.add_score(str(topic), str(pubmed_id), randint(1,10000))\n",
    "    \n",
    "    return _run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1: To what extent can bibliometric relevance signals be used as ranking criteria for TREC Precision Medicine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = '2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = Qrels.from_file(years_and_qrels[current_year]['PATH_QRELS'], kind=\"trec\")\n",
    "\n",
    "run_cite = get_run('TC', year=years_and_qrels[current_year]['YEAR']) # get run based on citations\n",
    "run_alt = get_run('ATTENTION_SCORE', year=years_and_qrels[current_year]['YEAR']) # get run based on attention score\n",
    "run_py = get_run('PY', year=years_and_qrels[current_year]['YEAR']) \n",
    "run_rl = get_run('RL', year=years_and_qrels[current_year]['YEAR']) \n",
    "run_if = get_run('IF', year=years_and_qrels[current_year]['YEAR']) \n",
    "\n",
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=[run_cite, run_alt, run_py, run_rl, run_if],\n",
    "    metrics=[\"recall\", \"ndcg\", \"map\", \"precision@10\", \"bpref\"],\n",
    "    rounding_digits=4,\n",
    "    max_p=0.05\n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(report.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random ranking based on qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_data = []\n",
    "cnt = 0\n",
    "trials = 10\n",
    "while cnt < trials:\n",
    "    run_random = get_random_run(qrels) \n",
    "    _df_data.append(evaluate(qrels, run_random,[\"recall\", \"ndcg\", \"map\", \"precision@10\", \"bpref\"]))\n",
    "    cnt += 1\n",
    "df = pd.DataFrame(_df_data)\n",
    "print(df.mean().to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBP comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=[run_cite, run_alt, run_py, run_rl, run_if],\n",
    "    metrics=[\"rbp.95\", \"rbp.9\", \"rbp.85\", \"rbp.8\", \"rbp.75\"],\n",
    "    rounding_digits=4,\n",
    "    max_p=0.05 \n",
    ")\n",
    "\n",
    "print(report)\n",
    "# print(report.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nDCG comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=[run_cite, run_alt, run_py, run_rl, run_if],\n",
    "    metrics=[\"ndcg@1\", \"ndcg@5\", \"ndcg@10\", \"ndcg@100\", \"ndcg@1000\"],\n",
    "    rounding_digits=4,\n",
    "    max_p=0.05  \n",
    ")\n",
    "\n",
    "print(report)\n",
    "# print(report.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different rank fusions of bibliometric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = 'rrf'\n",
    "\n",
    "fused_runs = []\n",
    "\n",
    "run_dict = {'C': run_cite,\n",
    "            'A': run_alt,\n",
    "            'P': run_py,\n",
    "            'R': run_rl,\n",
    "            'I': run_if}\n",
    "\n",
    "# https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of-a-list-s-elements\n",
    "import itertools\n",
    "\n",
    "# run_codes = ['a', 'b', 'c', 'd', 'e']\n",
    "run_codes = list(run_dict.keys())\n",
    "\n",
    "for L in range(len(run_codes) + 1):\n",
    "    for subset in itertools.combinations(run_codes, L):\n",
    "        if len(subset) > 1:\n",
    "            \n",
    "            run_name = ''.join(list(subset))\n",
    "            comb = [run_dict.get(code) for code in subset]\n",
    "            run_fuse = fuse(runs=comb, method=fuse_method)\n",
    "            run_fuse.name = run_name\n",
    "            fused_runs.append(run_fuse)\n",
    "                 \n",
    "# runs = [run_cite, run_alt, run_rl, run_if, run_py, run_random] + fused_runs\n",
    "  \n",
    "report = compare(\n",
    "    qrels=qrels,\n",
    "    runs=fused_runs,\n",
    "    metrics=[\"ndcg\", \"map\", \"precision@10\", \"bpref\"],\n",
    "    rounding_digits=4,\n",
    "    max_p=0.05 \n",
    ")\n",
    "\n",
    "print(report)\n",
    "print(report.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPRI heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(report.results)\n",
    "df = df.rename(index={\"ndcg\": \"nDCG\", \"map\": \"AP\", \"precision@10\": \"P@10\", \"bpref\": \"Bpref\"})\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "palette = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "ax = sns.heatmap(df.T, annot=True, fmt=\".4f\", linewidth=.5, cmap=palette)\n",
    "ax.xaxis.tick_top()\n",
    "file_path = ''.join(['figures/trec-pm-', current_year, '-abstracts/capri.rrf.heatmap.pdf'])\n",
    "plt.savefig(file_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2: Can bibliometric-enhanced rank fusion methods improve the overall retrieval performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = \"rrf\"\n",
    "\n",
    "df_data = []\n",
    "\n",
    "for root, dirs, files in os.walk(years_and_qrels[current_year]['DIR_RUN']):\n",
    "    for file in files:\n",
    "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
    "        \n",
    "        run_rrf = fuse(runs=[run, run_cite, run_alt, run_py], method=fuse_method)\n",
    "\n",
    "        report = compare(\n",
    "            qrels=qrels,\n",
    "            runs=[run, run_rrf],\n",
    "            metrics=[\"ndcg\", \"map\", \"precision@10\", \"bpref\", \n",
    "                     \"ndcg@10\", \"ndcg@50\", \"ndcg@100\", \"ndcg@200\", \"ndcg@500\", \"ndcg@1000\"],\n",
    "            max_p=0.05  \n",
    "        )\n",
    "\n",
    "        report_dict = report.to_dict()\n",
    "        metrics = report_dict.get('metrics')\n",
    "        base_name = report_dict.get('model_names')[0]\n",
    "        base_scores = report_dict.get(base_name).get('scores')\n",
    "        fuse_scores = report_dict.get(fuse_method).get('scores')\n",
    "        base_pval = report_dict.get(base_name).get('comparisons').get(fuse_method)\n",
    "\n",
    "        _eval = {'run': base_name}\n",
    "\n",
    "        for metric in metrics:\n",
    "            _eval[metric + ' (base)'] = base_scores.get(metric)\n",
    "            _eval[metric + ' (fuse)'] = fuse_scores.get(metric)\n",
    "            _eval[metric + ' (diff)'] = fuse_scores.get(metric) - base_scores.get(metric)\n",
    "            _eval[metric + ' (pval)'] = base_pval.get(metric)\n",
    "            \n",
    "        df_data.append(_eval)\n",
    "        \n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "os.makedirs(years_and_qrels[current_year]['DIR_CSV'], exist_ok=True)\n",
    "file_out = years_and_qrels[current_year]['DIR_CSV'] + fuse_method + '.csv'\n",
    "df.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nDCG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('experimental_results/trec-pm-2017-abstracts/rrf.csv')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20,5))\n",
    "\n",
    "df_data = {}\n",
    "for measure in [ 'ndcg@10', 'ndcg@50', 'ndcg@100', 'ndcg@200', 'ndcg@500', 'ndcg@1000']:\n",
    "    _data = {}\n",
    "    for thresh in [round(0.005*i, 3) for i in range(0,201)]:\n",
    "        _data[thresh] = len(df[(df[measure +' (base)'] > thresh)])\n",
    "    df_data[measure] = _data\n",
    "_df = pd.DataFrame.from_dict(df_data)\n",
    "_df.plot(linewidth=3, xlabel='Score', ylabel='Number of baseline systems \\n above score', ax=axes[0])\n",
    "axes[0].axhline(y = len(df), color='black',linestyle = '--', linewidth=3)\n",
    "\n",
    "df_data = {}\n",
    "for measure in ['ndcg@10', 'ndcg@50', 'ndcg@100', 'ndcg@200', 'ndcg@500', 'ndcg@1000']:\n",
    "    _data = {}\n",
    "    for thresh in [round(0.005*i, 3) for i in range(0,201)]:\n",
    "        _data[thresh] = len(df[(df[measure +' (fuse)'] > thresh)])\n",
    "    df_data[measure] = _data\n",
    "_df = pd.DataFrame.from_dict(df_data)\n",
    "_df.plot(linewidth=3, xlabel='Score', ylabel='Number of fused systems \\n above score', ax=axes[1])\n",
    "axes[1].axhline(y = len(df), color='black',linestyle = '--', linewidth=3)\n",
    "\n",
    "df_data = {}\n",
    "for measure in [ 'ndcg@10', 'ndcg@50', 'ndcg@100', 'ndcg@200', 'ndcg@500', 'ndcg@1000']:\n",
    "    _data = {}\n",
    "    for thresh in [round(0.005*i, 3) for i in range(0,201)]:\n",
    "        _data[thresh] = len(df[(df[measure +' (base)'] > thresh) & (df[measure +' (diff)'] > 0)])\n",
    "    df_data[measure] = _data\n",
    "_df = pd.DataFrame.from_dict(df_data)\n",
    "_df.plot(linewidth=3, xlabel='Score', ylabel='Number of baseline systems \\n that improve above score', ax=axes[2])\n",
    "axes[2].axhline(y = len(df), color='black',linestyle = '--', linewidth=3)\n",
    "\n",
    "df_data = {}\n",
    "for measure in [ 'ndcg@10', 'ndcg@50', 'ndcg@100', 'ndcg@200', 'ndcg@500', 'ndcg@1000']:\n",
    "    _data = {}\n",
    "    for thresh in [round(0.005*i, 3) for i in range(0,201)]:\n",
    "        _data[thresh] = len(df[(df[measure +' (fuse)'] > thresh) & (df[measure +' (diff)'] > 0)])\n",
    "    df_data[measure] = _data\n",
    "_df = pd.DataFrame.from_dict(df_data)\n",
    "_df.plot(linewidth=3, xlabel='Score', ylabel='Number of fused systems \\n that improved above score', ax=axes[3])\n",
    "axes[3].axhline(y = len(df), color='black',linestyle = '--', linewidth=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/trec-pm-2017-abstracts/ndcg.rrf.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC PM 2017 - Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('experimental_results/trec-pm-2017-abstracts/rrf.csv')\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "df.sort_values(by=['map (fuse)'], ascending=False).plot.bar(x='run', y='map (fuse)', ylabel=r'$\\Delta$ AP', xlabel='', legend=False, ax=axes[1]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['ndcg (diff)'], ascending=False).plot.bar(x='run', y='ndcg (diff)', ylabel=r'$\\Delta$ nDCG', xlabel='', legend=False, ax=axes[0]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['precision@10 (diff)'], ascending=False).plot.bar(x='run', y='precision@10 (diff)', ylabel=r'$\\Delta$ P@10', xlabel='', legend=False, ax=axes[2]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['bpref (diff)'], ascending=False).plot.bar(x='run', y='bpref (diff)', ylabel=r'$\\Delta$ Bpref', xlabel='', legend=False, ax=axes[3]).get_xaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/trec-pm-2017-abstracts/delta.rrf.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC PM 2018 - Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('experimental_results/trec-pm-2018-abstracts/rrf.csv')\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "df.sort_values(by=['map (fuse)'], ascending=False).plot.bar(x='run', y='map (fuse)', ylabel=r'$\\Delta$ AP', xlabel='', legend=False, ax=axes[1]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['ndcg (diff)'], ascending=False).plot.bar(x='run', y='ndcg (diff)', ylabel=r'$\\Delta$ nDCG', xlabel='', legend=False, ax=axes[0]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['precision@10 (diff)'], ascending=False).plot.bar(x='run', y='precision@10 (diff)', ylabel=r'$\\Delta$ P@10', xlabel='', legend=False, ax=axes[2]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['bpref (diff)'], ascending=False).plot.bar(x='run', y='bpref (diff)', ylabel=r'$\\Delta$ Bpref', xlabel='', legend=False, ax=axes[3]).get_xaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/trec-pm-2018-abstracts/delta.rrf.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC PM 2019 - Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('experimental_results/trec-pm-2019-abstracts/rrf.csv')\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "df.sort_values(by=['map (fuse)'], ascending=False).plot.bar(x='run', y='map (fuse)', ylabel=r'$\\Delta$ AP', xlabel='', legend=False, ax=axes[1]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['ndcg (diff)'], ascending=False).plot.bar(x='run', y='ndcg (diff)', ylabel=r'$\\Delta$ nDCG', xlabel='', legend=False, ax=axes[0]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['precision@10 (diff)'], ascending=False).plot.bar(x='run', y='precision@10 (diff)', ylabel=r'$\\Delta$ P@10', xlabel='', legend=False, ax=axes[2]).get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['bpref (diff)'], ascending=False).plot.bar(x='run', y='bpref (diff)', ylabel=r'$\\Delta$ Bpref', xlabel='', legend=False, ax=axes[3]).get_xaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/trec-pm-2019-abstracts/delta.rrf.pdf',bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC PM all years - Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat ( map (pd.read_csv, ['experimental_results/trec-pm-2017-abstracts/rrf.csv', 'experimental_results/trec-pm-2018-abstracts/rrf.csv', 'experimental_results/trec-pm-2019-abstracts/rrf.csv']),ignore_index=True)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,15))\n",
    "df.sort_values(by=['map (fuse)'], ascending=False).plot.bar(x='run', y='map (fuse)', ylabel=r'$\\Delta$ AP', xlabel='', legend=False, ax=axes[1], edgecolor=\"none\").get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['ndcg (diff)'], ascending=False).plot.bar(x='run', y='ndcg (diff)', ylabel=r'$\\Delta$ nDCG', xlabel='', legend=False, ax=axes[0], edgecolor=\"none\").get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['precision@10 (diff)'], ascending=False).plot.bar(x='run', y='precision@10 (diff)', ylabel=r'$\\Delta$ P@10', xlabel='', legend=False, ax=axes[2], edgecolor=\"none\").get_xaxis().set_visible(False)\n",
    "df.sort_values(by=['bpref (diff)'], ascending=False).plot.bar(x='run', y='bpref (diff)', ylabel=r'$\\Delta$ Bpref', xlabel='', legend=False, ax=axes[3], edgecolor=\"none\").get_xaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/all_years_delta.rrf.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Fusion Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see also: https://amenra.github.io/ranx/fusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selection ###\n",
    "\n",
    "fuse_method = fuse_dict_key =\"rbc\"\n",
    "# fuse_method = fuse_dict_key =\"bayesfuse\"\n",
    "# fuse_method = fuse_dict_key =\"probfuse\"\n",
    "\n",
    "#### score-based methods ####\n",
    "\n",
    "# fuse_method = \"anz\"; fuse_dict_key = \"comb_anz\"\n",
    "# fuse_method = \"max\"; fuse_dict_key = \"comb_max\"\n",
    "# fuse_method = \"med\"; fuse_dict_key = \"comb_med\"\n",
    "# fuse_method = \"min\"; fuse_dict_key = \"comb_min\"\n",
    "# fuse_method = \"mnz\"; fuse_dict_key = \"comb_mnz\"\n",
    "# fuse_method = \"sum\"; fuse_dict_key = \"comb_sum\"\n",
    "# fuse_method = \"gmnz\"; fuse_dict_key = \"comb_gmnz\"\n",
    "# fuse_method = \"mixed\"; fuse_dict_key = \"comb_mixed\"\n",
    "# fuse_method = \"wmnz\"; fuse_dict_key = \"wmnz\"\n",
    "# fuse_method = \"wsum\"; fuse_dict_key = \"wsum\"\n",
    "\n",
    "#### rank-based methods ####\n",
    "\n",
    "# fuse_method = fuse_dict_key = \"isr\"\n",
    "# fuse_method = fuse_dict_key = \"log_isr\"\n",
    "# fuse_method = fuse_dict_key = \"logn_isr\"\n",
    "# fuse_method = fuse_dict_key = \"rbc\"\n",
    "# fuse_method = fuse_dict_key = \"rrf\"\n",
    "\n",
    "#### probabilistic methods ####\n",
    "\n",
    "# fuse_method = fuse_dict_key = \"bayesfuse\"\n",
    "# fuse_method = fuse_dict_key = \"mapfuse\"\n",
    "# fuse_method = fuse_dict_key = \"posfuse\"\n",
    "# fuse_method = fuse_dict_key = \"probfuse\"\n",
    "# fuse_method = fuse_dict_key = \"segfuse\"\n",
    "# fuse_method = fuse_dict_key = \"slidefuse\"\n",
    "\n",
    "#### voting-based methods ####\n",
    "\n",
    "# fuse_method = fuse_dict_key = \"bordafuse\"\n",
    "# fuse_method = fuse_dict_key = \"w_bordafuse\"\n",
    "# fuse_method = fuse_dict_key = \"condorcet\"\n",
    "# fuse_method = fuse_dict_key = \"w_condorcet\"\n",
    "\n",
    "skip_list = [\"bordafuse\", \"condorcet\", \"mnz\", \"sum\", \"min\", \"anz\", \"med\", \"max\", \"isr\", \"log_isr\"]\n",
    "\n",
    "if years_and_qrels[current_year]['YEAR'] == 2017:\n",
    "    samples = 5\n",
    "    dir_train = 'runs/trec-pm-2018-abstracts/'\n",
    "    train_runs = [Run.from_file(dir_train + random.choice(os.listdir(dir_train)), kind='trec') for i in range(0,samples)]\n",
    "    train_qrels = Qrels.from_file('trec-pm/trec-pm-2018-abstracts/qrels-treceval-abstracts-2018-v2.txt', kind=\"trec\")\n",
    "    \n",
    "if years_and_qrels[current_year]['YEAR'] == 2018:\n",
    "    samples = 5\n",
    "    dir_train = 'runs/trec-pm-2019-abstracts/'\n",
    "    train_runs = [Run.from_file(dir_train + random.choice(os.listdir(dir_train)), kind='trec') for i in range(0,samples)]\n",
    "    train_qrels = Qrels.from_file('trec-pm/trec-pm-2019-abstracts/qrels-treceval-abstracts.2019.txt', kind=\"trec\")\n",
    "\n",
    "if years_and_qrels[current_year]['YEAR'] == 2019:\n",
    "    samples = 5\n",
    "    dir_train = 'runs/trec-pm-2017-abstracts/'\n",
    "    train_runs = [Run.from_file(dir_train + random.choice(os.listdir(dir_train)), kind='trec') for i in range(0,samples)]\n",
    "    train_qrels = Qrels.from_file('trec-pm/trec-pm-2017-abstracts/qrels-final-abstracts.txt', kind=\"trec\")\n",
    "\n",
    "if fuse_method not in skip_list:\n",
    "\n",
    "    best_params = optimize_fusion(\n",
    "        qrels=train_qrels,\n",
    "        runs=train_runs,\n",
    "        norm=\"min-max\",\n",
    "        method=fuse_method,\n",
    "        metric=\"ndcg@100\"\n",
    "    )\n",
    "\n",
    "run_cite = get_run('TC', year=years_and_qrels[current_year]['YEAR']) # get run based on citations\n",
    "run_alt = get_run('ATTENTION_SCORE', year=years_and_qrels[current_year]['YEAR']) # get run based on attention score\n",
    "run_py = get_run('PY', year=years_and_qrels[current_year]['YEAR']) # get run based on publication year\n",
    "qrels = Qrels.from_file(years_and_qrels[current_year]['PATH_QRELS'], kind=\"trec\")\n",
    "\n",
    "df_data = []\n",
    "\n",
    "for root, dirs, files in os.walk(years_and_qrels[current_year]['DIR_RUN']):\n",
    "    for file in files:\n",
    "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
    "        \n",
    "        if fuse_method in skip_list:\n",
    "            run_fuse = fuse(\n",
    "                runs=[run, run_cite, run_alt, run_py],       \n",
    "                method=fuse_method     \n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            run_fuse = fuse(\n",
    "                runs=[run, run_cite, run_alt, run_py],  \n",
    "                norm=\"min-max\",       \n",
    "                method=fuse_method,        \n",
    "                params=best_params,\n",
    "            )\n",
    "\n",
    "        report = compare(\n",
    "            qrels=qrels,\n",
    "            runs=[run, run_fuse],\n",
    "            metrics=[\"ndcg\", \"map\", \"precision@10\", \"bpref\"],\n",
    "            max_p=0.05  \n",
    "        )\n",
    "\n",
    "        report_dict = report.to_dict()\n",
    "        metrics = report_dict.get('metrics')\n",
    "        base_name = report_dict.get('model_names')[0]\n",
    "        base_scores = report_dict.get(base_name).get('scores')\n",
    "        fuse_scores = report_dict.get(fuse_dict_key).get('scores')\n",
    "        base_pval = report_dict.get(base_name).get('comparisons').get(fuse_dict_key)\n",
    "\n",
    "        _eval = {'run': base_name}\n",
    "\n",
    "        for metric in metrics:\n",
    "            _eval[metric + ' (base)'] = base_scores.get(metric)\n",
    "            _eval[metric + ' (fuse)'] = fuse_scores.get(metric)\n",
    "            _eval[metric + ' (diff)'] = fuse_scores.get(metric) - base_scores.get(metric)\n",
    "            _eval[metric + ' (pval)'] = base_pval.get(metric)\n",
    "            \n",
    "        df_data.append(_eval)\n",
    "        \n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "os.makedirs(years_and_qrels[current_year]['DIR_CSV'], exist_ok=True)\n",
    "file_out = years_and_qrels[current_year]['DIR_CSV'] + fuse_method + '.csv'\n",
    "df.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General statistics about how rank fusion changes the retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_stats(method):\n",
    "    data = {}\n",
    "    for year in [2017, 2018, 2019]:\n",
    "        \n",
    "        path = 'experimental_results/trec-pm-' + str(year) + '-abstracts/' + method + '.csv'\n",
    "        df = pd.read_csv(path)\n",
    "        _data = {\n",
    "            'Number of systems': len(df),\n",
    "            '(Signif.*) improvements (nDCG)': str(len(df[df['ndcg (diff)'] > 0])) + ' / ' + str(len(df[(df['ndcg (pval)'] < 0.05) & (df['ndcg (diff)'] > 0)])) + '*',\n",
    "            #'Improvements (nDCG)': int(len(df[df['ndcg (diff)'] > 0])),\n",
    "            #'Signif. improvements (nDCG)': len(df[(df['ndcg (pval)'] < 0.05) & (df['ndcg (diff)'] > 0)]),\n",
    "            'Average improvement (nDCG)': df[(df['ndcg (pval)'] < 0.05) & (df['ndcg (diff)'] > 0)]['ndcg (diff)'].mean(),\n",
    "            'Overall change (nDCG)': df['ndcg (diff)'].mean(),\n",
    "            '(Signif.*) improvements (AP)': str(len(df[df['map (diff)'] > 0])) + ' / ' + str(len(df[(df['map (pval)'] < 0.05) & (df['map (diff)'] > 0)])) + '*',\n",
    "            #'Improvements (AP)': len(df[df['map (diff)'] > 0]),\n",
    "            #'Signif. improvements (AP)': len(df[(df['map (pval)'] < 0.05) & (df['map (diff)'] > 0)]),\n",
    "            'Average improvement (AP)': df[(df['map (pval)'] < 0.05) & (df['map (diff)'] > 0)]['map (diff)'].mean(),\n",
    "            'Overall change (AP)': df['map (diff)'].mean(),\n",
    "            '(Signif.*) improvements (P@10)': str(len(df[df['precision@10 (diff)'] > 0])) + ' / ' + str(len(df[(df['precision@10 (pval)'] < 0.05) & (df['precision@10 (diff)'] > 0)])) + '*',\n",
    "            #'Improvements (P@10)': len(df[df['precision@10 (diff)'] > 0]),\n",
    "            #'Signif. improvements (P@10)': len(df[(df['precision@10 (pval)'] < 0.05) & (df['precision@10 (diff)'] > 0)]),\n",
    "            'Average improvement (P@10)': df[(df['precision@10 (pval)'] < 0.05) & (df['precision@10 (diff)'] > 0)]['precision@10 (diff)'].mean(),\n",
    "            'Overall change (P@10)': df['precision@10 (diff)'].mean(),\n",
    "            '(Signif.*) improvements (Bpref)': str(len(df[df['bpref (diff)'] > 0])) + ' / ' + str(len(df[(df['bpref (pval)'] < 0.05) & (df['bpref (diff)'] > 0)])) + '*',\n",
    "            #'Improvements (Bpref)': len(df[df['bpref (diff)'] > 0]),\n",
    "            #'Signif. improvements (Bpref)': len(df[(df['bpref (pval)'] < 0.05) & (df['bpref (diff)'] > 0)]),\n",
    "            'Average improvement (Bpref)': df[(df['bpref (pval)'] < 0.05) & (df['bpref (diff)'] > 0)]['bpref (diff)'].mean(),\n",
    "            'Overall change (Bpref)': df['bpref (diff)'].mean()\n",
    "        }\n",
    "        data[year] = _data\n",
    "\n",
    "    return pd.DataFrame.from_dict(data).reindex(_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_stats('rrf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_stats('rbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_stats('bayesfuse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProbFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_stats('probfuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = general_stats('rrf')\n",
    "df = pd.concat([df, general_stats('rbc').reindex(df.index)], axis=1)\n",
    "df = pd.concat([df, general_stats('bayesfuse').reindex(df.index)], axis=1)\n",
    "df = pd.concat([df, general_stats('probfuse').reindex(df.index)], axis=1)\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3: How do users benefit from bibliometric-enhanced rank fusion results?\n",
    "\n",
    "The RBP experiments show: the more patient the user, the higher benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_method = \"rrf\"\n",
    "\n",
    "_metrics = ['rbp.' + str(round(0.8 + 0.005*i, 3)).replace('0.', '') for i in range(0,40)]\n",
    "\n",
    "df_data = []\n",
    "\n",
    "for root, dirs, files in os.walk(years_and_qrels[current_year]['DIR_RUN']):\n",
    "    for file in files:\n",
    "        run = Run.from_file(os.path.join(root, file), kind=\"trec\")\n",
    "        \n",
    "        run_rrf = fuse(runs=[run, run_cite, run_alt, run_py], method=fuse_method)\n",
    "\n",
    "        report = compare(\n",
    "            qrels=qrels,\n",
    "            runs=[run, run_rrf],\n",
    "            metrics=_metrics,\n",
    "            max_p=0.05  \n",
    "        )\n",
    "\n",
    "        report_dict = report.to_dict()\n",
    "        metrics = report_dict.get('metrics')\n",
    "        base_name = report_dict.get('model_names')[0]\n",
    "        base_scores = report_dict.get(base_name).get('scores')\n",
    "        fuse_scores = report_dict.get(fuse_method).get('scores')\n",
    "        base_pval = report_dict.get(base_name).get('comparisons').get(fuse_method)\n",
    "\n",
    "        _eval = {'run': base_name}\n",
    "\n",
    "        for metric in metrics:\n",
    "            _eval[metric + ' (base)'] = base_scores.get(metric)\n",
    "            _eval[metric + ' (fuse)'] = fuse_scores.get(metric)\n",
    "            _eval[metric + ' (diff)'] = fuse_scores.get(metric) - base_scores.get(metric)\n",
    "            _eval[metric + ' (pval)'] = base_pval.get(metric)\n",
    "            \n",
    "        df_data.append(_eval)\n",
    "        \n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "os.makedirs(years_and_qrels[current_year]['DIR_CSV'], exist_ok=True)\n",
    "file_out = years_and_qrels[current_year]['DIR_CSV'] + fuse_method + '.rbp.csv'\n",
    "df.to_csv(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv('experimental_results/trec-pm-2017-abstracts/rrf.rbp.csv')\n",
    "df_2018 = pd.read_csv('experimental_results/trec-pm-2018-abstracts/rrf.rbp.csv')\n",
    "df_2019 = pd.read_csv('experimental_results/trec-pm-2019-abstracts/rrf.rbp.csv')\n",
    "\n",
    "_data_2017 = {}\n",
    "_data_2018 = {}\n",
    "_data_2019 = {}\n",
    "_range = [round(0.8 + 0.005*i, 3) for i in range(0,40)]\n",
    "for p in _range:\n",
    "    _data_2017[p] = len(df_2017[df_2017['rbp.' + str(p).replace('0.', '') +' (diff)'] > 0])\n",
    "    _data_2018[p] = len(df_2018[df_2018['rbp.' + str(p).replace('0.', '') +' (diff)'] > 0])\n",
    "    _data_2019[p] = len(df_2019[df_2019['rbp.' + str(p).replace('0.', '') +' (diff)'] > 0])\n",
    "pd.DataFrame.from_dict({'TREC PM 2017': _data_2017, 'TREC PM 2018': _data_2018, 'TREC PM 2019': _data_2019}).plot(xlabel='p', ylabel='Number of systems that improve', figsize=(5,5), marker='o', linewidth=3, markevery=2)\n",
    "plt.axhline(y = len(df_2017), color='tab:blue',linestyle = '--', linewidth=3)\n",
    "plt.axhline(y = len(df_2018), color='tab:orange',linestyle = '--', linewidth=3)\n",
    "plt.axhline(y = len(df_2019), color='tab:green',linestyle = '--', linewidth=3)\n",
    "\n",
    "plt.savefig('figures/rbp.rrf.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd37a288dadd1a143fed9043b8d48ccc5bda2a214fe6a38102e219830c3263df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
